{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seldon Core MLFlow Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seldon is used to containerise `trained machine learning models` and deploy them into Kubernetes environments. Seldon can be used to compose `complex inference pipelines` to orchestrate multiple components including models, data transformers, combiners, drift detectors, outlier detectors and explainers for advanced monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a trained ML model to deploy. This will need to be stored in an s3 compatible bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUri = gs://seldon-models/mlflow/elasticnet_wine_1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Server (container)\n",
    "\n",
    "We now need to provide a container to load our trained model and run predict. Seldon has some pre-packaged servers (Tensorflow, XGBoost, scikit-learn and MLFlow) otherwise we need to build our own.\n",
    "\n",
    "For mlflow use-case the following file provides the logic for how we will load and run predictions with the trained model. We can build a container from this `MLFlowServer.py` file by:\n",
    "- creating requirements file\n",
    "- creating environment OR Docker file\n",
    "- building a container from Seldon base image with s2i OR Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MLFlowServer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import requests\n",
    "from mlflow import pyfunc\n",
    "from seldon_core import Storage\n",
    "from seldon_core.user_model import SeldonComponent\n",
    "from typing import Dict, List, Union, Iterable\n",
    "\n",
    "log = logging.getLogger()\n",
    "\n",
    "MLFLOW_SERVER = \"model\"\n",
    "\n",
    "\n",
    "class MLFlowServer(SeldonComponent):\n",
    "    def __init__(self, model_uri: str):\n",
    "        super().__init__()\n",
    "        log.info(f\"Creating MLFLow server with URI {model_uri}\")\n",
    "        self.model_uri = model_uri\n",
    "        self.ready = False\n",
    "\n",
    "    def load(self):\n",
    "        log.info(f\"Downloading model from {self.model_uri}\")\n",
    "        model_folder = Storage.download(self.model_uri)\n",
    "        self._model = pyfunc.load_model(model_folder)\n",
    "        self.ready = True\n",
    "\n",
    "    def predict(\n",
    "        self, X: np.ndarray, feature_names: Iterable[str] = [], meta: Dict = None\n",
    "    ) -> Union[np.ndarray, List, Dict, str, bytes]:\n",
    "        log.info(f\"Requesting prediction with: {X}\")\n",
    "\n",
    "        if not self.ready:\n",
    "            raise requests.HTTPError(\"Model not loaded yet\")\n",
    "\n",
    "        result = self._model.predict(X)\n",
    "        log.info(f\"Prediction result: {result}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple inference graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: mlflow\n",
    "spec:\n",
    "  name: wines\n",
    "  predictors:\n",
    "  - componentSpecs:\n",
    "    - spec:\n",
    "        containers:\n",
    "        - name: classifier\n",
    "          livenessProbe:\n",
    "            initialDelaySeconds: 80\n",
    "            failureThreshold: 200\n",
    "            periodSeconds: 5\n",
    "            successThreshold: 1\n",
    "            httpGet:\n",
    "              path: /health/ping\n",
    "              port: http\n",
    "              scheme: HTTP\n",
    "          readinessProbe:\n",
    "            initialDelaySeconds: 80\n",
    "            failureThreshold: 200\n",
    "            periodSeconds: 5\n",
    "            successThreshold: 1\n",
    "            httpGet:\n",
    "              path: /health/ping\n",
    "              port: http\n",
    "              scheme: HTTP\n",
    "    graph:\n",
    "      children: []\n",
    "      implementation: MLFLOW_SERVER\n",
    "      modelUri: gs://seldon-models/mlflow/elasticnet_wine_1.8.0\n",
    "      name: classifier\n",
    "    name: default\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/mlflow unchanged\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                           READY   STATUS    RESTARTS   AGE\r\n",
      "mlflow-default-0-classifier-5c9dcfc855-647hn   2/2     Running   0          128m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[],\"ndarray\":[5.477889635651638]},\"meta\":{\"requestPath\":{\"classifier\":\"seldonio/mlflowserver:1.9.1\"}}}\r\n"
     ]
    }
   ],
   "source": [
    "!curl -s -d '{\"data\": {\"names\": [], \"ndarray\": [[6.2, 0.270, 0.43, 7.80, 0.056, 48.0, 244.0, 0.99560, 3.10, 0.51, 10.00]]}}' \\\n",
    "   -X POST http://34.90.29.195/seldon/default/mlflow/api/v1.0/predictions \\\n",
    "   -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Seldon Core capabilities \n",
    "    \n",
    "- metadata    \n",
    "- custom metrics \n",
    "- infrastructure and performance monitoring with Prometheus\n",
    "- visualisation with Grafana\n",
    "- logging request, responses and container logs with ElasticSearch\n",
    "- tracing with Jaeger for latency \n",
    "\n",
    "![title](seldoncore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges \n",
    "\n",
    "The main challenges with adopting Seldon Core at enterprise scale are: \n",
    "\n",
    "1. requiring Kubernetes skills to interface with the platform\n",
    "\n",
    "\n",
    "2. requires building integrations with open source tools for logging and monitoring such as Prometheus, Grafana and ElasticSearch \n",
    "\n",
    "\n",
    "3. would require building out additional features for auth, permissioning, model artefact registration and visualisation of outliers, drift and explainers "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
