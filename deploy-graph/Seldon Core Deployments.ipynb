{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seldon Core Income Classifier Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seldon is used to containerise `trained machine learning models` and deploy them into Kubernetes environments. Seldon can be used to compose `complex inference pipelines` to orchestrate multiple components including models, data transformers, combiners, drift detectors, outlier detectors and explainers for advanced monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a trained ML model to deploy. This will need to be stored in an s3 compatible bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelUri = gs://seldon-models/sklearn/income/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Server (container)\n",
    "\n",
    "We now need to provide a container to load our trained model and run predict. Seldon has some pre-packaged servers (Tensorflow, XGBoost, scikit-learn and MLFlow) otherwise we need to build our own.\n",
    "\n",
    "For mlflow use-case the following file provides the logic for how we will load and run predictions with the trained model. We can build a container from this `SklearnServer.py` file by:\n",
    "- creating requirements file\n",
    "- creating environment OR Docker file\n",
    "- building a container from Seldon base image with s2i OR Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SklearnServer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import seldon_core\n",
    "from seldon_core.user_model import SeldonComponent\n",
    "from typing import Dict, List, Union, Iterable\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "JOBLIB_FILE = \"model.joblib\"\n",
    "\n",
    "\n",
    "class SKLearnServer(SeldonComponent):\n",
    "    def __init__(self, model_uri: str = None, method: str = \"predict_proba\"):\n",
    "        super().__init__()\n",
    "        self.model_uri = model_uri\n",
    "        self.method = method\n",
    "        self.ready = False\n",
    "        logger.info(f\"Model uri: {self.model_uri}\")\n",
    "        logger.info(f\"method: {self.method}\")\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        logger.info(\"load\")\n",
    "        model_file = os.path.join(\n",
    "            seldon_core.Storage.download(self.model_uri), JOBLIB_FILE\n",
    "        )\n",
    "        logger.info(f\"model file: {model_file}\")\n",
    "        self._joblib = joblib.load(model_file)\n",
    "        self.ready = True\n",
    "\n",
    "    def predict(\n",
    "        self, X: np.ndarray, names: Iterable[str], meta: Dict = None\n",
    "    ) -> Union[np.ndarray, List, str, bytes]:\n",
    "        try:\n",
    "            if not self.ready:\n",
    "                self.load()\n",
    "            if self.method == \"predict_proba\":\n",
    "                logger.info(\"Calling predict_proba\")\n",
    "                result = self._joblib.predict_proba(X)\n",
    "            elif self.method == \"decision_function\":\n",
    "                logger.info(\"Calling decision_function\")\n",
    "                result = self._joblib.decision_function(X)\n",
    "            else:\n",
    "                logger.info(\"Calling predict\")\n",
    "                result = self._joblib.predict(X)\n",
    "            return result\n",
    "        except Exception as ex:\n",
    "            logging.exception(\"Exception during predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple inference graph\n",
    "\n",
    "In Seldon we use Custom Resource Definitions (CRDs) to define inference logic. CRDs are extensions of the K8s API allowing us to create a combo of K8s objects that we can orchestrate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: income\n",
    "spec:\n",
    "  name: income\n",
    "  annotations:\n",
    "    seldon.io/rest-timeout: \"100000\"\n",
    "  predictors:\n",
    "  - graph:\n",
    "      children: []\n",
    "      implementation: SKLEARN_SERVER\n",
    "      modelUri: gs://seldon-models/sklearn/income/model-0.23.2\n",
    "      name: classifier\n",
    "    name: default\n",
    "    replicas: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f deployment.yaml -n test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -n test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the income classifier model running as a production ready REST/gRPC microservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s -d '{\"data\": {\"names\": [], \"ndarray\": [[39, 7, 1, 1, 1, 1, 4, 1, 2174, 0, 40, 9]]}}' \\\n",
    "   -X POST http://34.141.246.254/seldon/test/income/api/v1.0/predictions \\\n",
    "   -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Seldon Core capabilities \n",
    "    \n",
    "- complex inference pipelines with predictors, transformers, combiners, outlier detectors, drift detectors and explainers\n",
    "- metadata    \n",
    "- custom metrics \n",
    "- infrastructure and performance monitoring with Prometheus\n",
    "- visualisation with Grafana\n",
    "- logging request, responses and container logs with ElasticSearch\n",
    "- tracing with Jaeger for latency \n",
    "\n",
    "![title](seldoncore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges \n",
    "\n",
    "The main challenges with adopting Seldon Core at enterprise scale are: \n",
    "\n",
    "1. requiring Kubernetes skills to interface with the platform\n",
    "\n",
    "\n",
    "2. requires building integrations with open source tools for logging and monitoring such as Prometheus, Grafana and ElasticSearch \n",
    "\n",
    "\n",
    "3. would require building out additional features for auth, permissioning, model artefact registration and visualisation of features, predictions, outliers, drift and explainers "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
